{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW--CbngmopF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6fa012b-ba69-4905-f6e4-4f1c6af0c31c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 69068379.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 71677925.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 83995960.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11387046.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.1542, Train Accuracy: 0.9521\n",
            "Epoch 2/10, Train Loss: 0.0435, Train Accuracy: 0.9865\n",
            "Epoch 3/10, Train Loss: 0.0293, Train Accuracy: 0.9908\n",
            "Epoch 4/10, Train Loss: 0.0208, Train Accuracy: 0.9936\n",
            "Epoch 5/10, Train Loss: 0.0163, Train Accuracy: 0.9946\n",
            "Epoch 6/10, Train Loss: 0.0117, Train Accuracy: 0.9962\n",
            "Epoch 7/10, Train Loss: 0.0098, Train Accuracy: 0.9966\n",
            "Epoch 8/10, Train Loss: 0.0080, Train Accuracy: 0.9973\n",
            "Epoch 9/10, Train Loss: 0.0063, Train Accuracy: 0.9978\n",
            "Epoch 10/10, Train Loss: 0.0071, Train Accuracy: 0.9974\n"
          ]
        }
      ],
      "source": [
        "#### this is our cnn model for mnist dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the network architecture\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x1=x\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.linear2(x)\n",
        "        return x,x1\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
        "\n",
        "# Define the data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create an instance of the network\n",
        "model = Network().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs,x1 = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        train_acc += accuracy_score(labels.cpu(), predictions.cpu()) * images.size(0)\n",
        "\n",
        "    train_loss = train_loss / len(train_dataset)\n",
        "    train_acc = train_acc / len(train_dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "test_loss = 0.0\n",
        "test_acc = 0.0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs,x1 = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        test_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Training part and storing by appending\n",
        "\n",
        "\n",
        "\n",
        "# Initialize empty lists for x_train and y_train\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs,x1 = model(images)  # Get the intermediate feature maps (x1) and final outputs (x)\n",
        "\n",
        "        # Append the intermediate feature maps and labels to x_train and y_train respectively\n",
        "        x_train.append(x1)\n",
        "        y_train.append(labels)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        train_acc += accuracy_score(labels.cpu(), predictions.cpu()) * images.size(0)\n",
        "\n",
        "    train_loss = train_loss / len(train_dataset)\n",
        "    train_acc = train_acc / len(train_dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')"
      ],
      "metadata": {
        "id": "H2-1EhcSnANy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Testing part and appending it\n",
        "\n",
        "\n",
        "\n",
        "# Initialize empty lists for x_test and y_test\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "# Evaluation loop\n",
        "test_loss = 0.0\n",
        "test_acc = 0.0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs,x1 = model(images)  # Get the intermediate feature maps (x1) and final outputs (x)\n",
        "\n",
        "        # Append the intermediate feature maps and labels to x_test and y_test respectively\n",
        "        x_test.append(x1)\n",
        "        y_test.append(labels)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        test_acc += accuracy_score(labels.cpu(), predictions.cpu()) * images.size(0)\n",
        "\n",
        "test_loss = test_loss / len(test_dataset)\n",
        "test_acc = test_acc / len(test_dataset)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "xHe8IYbLnlTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a51HcbBboT7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### imbalance dataloader part\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, WeightedRandomSampler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the network architecture\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x1=x\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.linear2(x)\n",
        "        return x,x1\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
        "\n",
        "# Define the data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create an instance of the network\n",
        "model = Network().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs,x1 = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        train_acc += accuracy_score(labels.cpu(), predictions.cpu()) * images.size(0)\n",
        "\n",
        "    train_loss = train_loss / len(train_dataset)\n",
        "    train_acc = train_acc / len(train_dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "test_loss = 0.0\n",
        "test_acc = 0.0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs,x1 = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        test_acc\n",
        "\n",
        "\n",
        "\n",
        "def get_imbalanced_data(dataset, num_sample_per_class, shuffle=False, random_seed=0):\n",
        "    \"\"\"\n",
        "    Return a list of imbalanced indices from a dataset.\n",
        "    Input: A dataset (e.g., CIFAR-10), num_sample_per_class: list of integers\n",
        "    Output: imbalanced_list\n",
        "    \"\"\"\n",
        "    length = dataset.__len__()\n",
        "    num_sample_per_class = list(num_sample_per_class)\n",
        "    selected_list = []\n",
        "    indices = list(range(0,length))\n",
        "\n",
        "    for i in range(0, length):\n",
        "        index = indices[i]\n",
        "        _, label = dataset.__getitem__(index)\n",
        "        if num_sample_per_class[label] > 0:\n",
        "            selected_list.append(index)\n",
        "            num_sample_per_class[label] -= 1\n",
        "\n",
        "    return selected_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_sample_per_class = [4500, 2000, 1000, 800, 600, 500, 400, 250, 150, 80]\n",
        "\n",
        "batch_size = 16\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5))])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "num_sample_per_class = [4500, 2000, 1000, 800, 600, 500, 400, 250, 150, 80]\n",
        "\n",
        "train_in_idx = get_imbalanced_data(trainset, num_sample_per_class)\n",
        "trainloader1 = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=SubsetRandomSampler(train_in_idx), num_workers=8)\n",
        "\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "num_sample_per_class1 = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
        "test_in_idx = get_imbalanced_data(testset, num_sample_per_class1)\n",
        "testloader1 = torch.utils.data.DataLoader(testset, batch_size=batch_size, sampler=SubsetRandomSampler(test_in_idx), num_workers=8)\n",
        "\n",
        "\n",
        "\n",
        "model = Network().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in trainloader1 :\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs,x1 = model(images)\n",
        "        loss = criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        train_acc += accuracy_score(labels.cpu(), predictions.cpu()) * images.size(0)\n",
        "\n",
        "    train_loss = train_loss / len(trainloader1 )\n",
        "    train_acc = train_acc / len(trainloader1 )\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
        "\n",
        "# Evaluation loop\n",
        "test_loss = 0.0\n",
        "test_acc = 0.0\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u81nGjjaFEtg",
        "outputId": "4a8fc9da-ff48-44e4-859a-8b8cf3214a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.1524, Train Accuracy: 0.9517\n",
            "Epoch 2/10, Train Loss: 0.0440, Train Accuracy: 0.9863\n",
            "Epoch 3/10, Train Loss: 0.0293, Train Accuracy: 0.9905\n",
            "Epoch 4/10, Train Loss: 0.0204, Train Accuracy: 0.9936\n",
            "Epoch 5/10, Train Loss: 0.0151, Train Accuracy: 0.9949\n",
            "Epoch 6/10, Train Loss: 0.0111, Train Accuracy: 0.9964\n",
            "Epoch 7/10, Train Loss: 0.0104, Train Accuracy: 0.9963\n",
            "Epoch 8/10, Train Loss: 0.0080, Train Accuracy: 0.9976\n",
            "Epoch 9/10, Train Loss: 0.0066, Train Accuracy: 0.9978\n",
            "Epoch 10/10, Train Loss: 0.0062, Train Accuracy: 0.9980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 2.9875, Train Accuracy: 15.0715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Train Loss: 0.8788, Train Accuracy: 15.7185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Train Loss: 0.4153, Train Accuracy: 15.8616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Train Loss: 0.3574, Train Accuracy: 15.8663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Train Loss: 0.2085, Train Accuracy: 15.9176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Train Loss: 0.2815, Train Accuracy: 15.8958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Train Loss: 0.1346, Train Accuracy: 15.9425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Train Loss: 0.0305, Train Accuracy: 15.9782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Train Loss: 0.1755, Train Accuracy: 15.9425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Train Loss: 0.2416, Train Accuracy: 15.9191\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear1): Linear(in_features=3136, out_features=512, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbrcFbzPpPch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}